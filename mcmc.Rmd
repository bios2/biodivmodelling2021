---
title: "Rejection Sampling"
author: "Dominique Gravel"
date: "August 26th, 2021"
output:
  xaringan::moon_reader:
    css: [default, assets/ecl707.css, "hygge"]
    lib_dir: assets
    seal: false
    nature:
      highlightStyle: monokai
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
---

class: title-slide, middle

<style type="text/css">
  .title-slide {
    background-image: url('assets/img/bg.jpg');
    background-color: #23373B;
    background-size: contain;
    border: 0px;
    background-position: 600px 0;
    line-height: 1;
  }
</style>

# Sampling the posterior with MCMC

<hr width="65%" align="left" size="0.3" color="orange"></hr>

## Rejection sampling, Metropolis Hastings etc.

<hr width="65%" align="left" size="0.3" color="orange" style="margin-bottom:40px;" alt="@Martin Sanchez"></hr>

.instructors[
  **ECL707/807** - Dominique Gravel
]

<img src="assets/img/logo.png" width="25%" style="margin-top:20px;"></img>

---
# Bayes theorem 

$$
\begin{align}
P(\theta|D,H) &= \frac{P(D|\theta,H) P(\theta,H)}{P(D|H)} \\
posterior &= \frac{likelihood \times prior}{evidence}
\end{align}
$$

The problem : how can we compute the posterior distribution without knowledge of the denominator ? 

---

# Dealing with the normalization constant 

1. Use the proportional form : 

$$ P[\theta \mid X] \propto P[X \mid \theta]P[\theta] $$

2. Use conjugacy 

3. Sample from the posterior 

---

# Rejection sampling 

1. Use an easily sampled distribution (the candidate distribution $c(x)$ )
2. Evaluate the probability of the candidate according to a target distribution $t(x)$
3. Reject candidates with a probability proportional to the difference between the two distributions

---
# Rejection sampling 

** Pseudo-code**

```
DEFINE candidate distribution c(x)
DEFINE target distribution t(x)
DEFINE constant M such that M*c(x) >= t(x) for all x
```

---

# Rejection sampling 

Acceptance probability : 

$$p = \frac{t(x)}{Mc(x)} $$

---

# Rejection sampling
# Pseudo-code (next)

```
REPEAT 
	DRAW sample X from c(x)
	COMPUTE acceptance probability p = t(X)/Mc(X)
	DRAW random value rand from uniform on [0,1] 	
	IF rand < p 
		accept x
	ELSE 
		reject X
UNTIL X is accepted 
```

---

# Rejection sampling 
## Exercise 1

Write a function that uses rejection sampling to return $n$ random normal deviates using only a uniform random number generator as a source of randomness. 

Note : you can still use *dnorm* to evaluate your target distribution. 

---

# Rejection sampling 
## Exercise 1

```{r, echo = FALSE, eval = FALSE}	
	rej_norm <- function(n, mu, sig) {
		# Define the target
		target <- function(x, mu, sig) dnorm(x)
		# Define the candidate
		cand <- function(x) dunif(x,-2*sig,2*sig)
		# Constant
		M <- 5
		# Object to store the n results 
		result <- numeric(n)
		# Main loop
		for(i in 1:n) {
		    accept <- FALSE
		    while(!accept) {
		      X <- runif(1,-6,6)
		      p <- target(X) / (M * cand(X))
		      P <- runif(1)
		      accept <- (P < p) 
			  } #endwhile
		    result[i] <- X 
		} #endfor
		return(result) } #endfunction
```

---

# Metropolis-Hastings algorithm

- Simulated annealing is a special case of MH
- Unlike rejection sampling, each iteration generates a sample from the target distribution
- Samples are dependent (autocorrelated), so effective sample size is much smaller than chain length
- For a Markov Chain $X$, draw $X_t$ from candidate distribution $j(X|X_{t-1})$ and compare to target distribution $f(x)$
- Better propositions are not deterministically accepted
- The posterior probability is stationary

--- 

# Metropolis-Hastings algorithm
## Jump directly to the R code 

```{r, echo = FALSE, eval = FALSE}	
	metropolis <- function(N = 5000, X0 = 0, A) #{
		# N: number of MCMC iterations
		# X0: starting value of the chain 
		# A: step parameter
		# Object to store results
		chain <- numeric(N + 1)
		chain[1] <- X0
	#}
```

--- 

# Metropolis-Hastings algorithm
## Jump directly to the R code 

```{r, echo = FALSE, eval = FALSE}	
	metropolis <- function(N = 5000, X0 = 0, A) #{
		# DEFINE target distribution 
		cand_fn <- function(y, A) runif(1, y-A, y+A)

		# DEFINE candidate distribution
		target_fn <- dnorm(x, 0, 1)

		# DEFINE acceptance probability function
		accept_fn <- function(current.value, previous.value) 
			target_fn(current.value)/target_fn(previous.value)

```

--- 

# Metropolis-Hastings algorithm
## Jump directly to the R code 

```{r, echo = FALSE, eval = FALSE}	
	metropolis <- function(N = 5000, X0 = 0, A) #{

		for(step in 2:(N+1)) {
			# PROPOSE candidate
			current.value <- cand_fn(chain[step -1], A)

			# COMPUTE acceptance probability
			p <- accept_fn(current.value, chain[step-1])

			# DO rejection sampling
			rand <- runif(1)
			if(rand <- p) 
				chain[t] = current.value
			else 
				chain[t] = chain[t-1]
		}

		#return(chain)
	#}
```

---

# Metropolis-Hastings algorithm
## Exercise 2. 

Copy the MH code and adapt it to simulate a binomial distribution. 
